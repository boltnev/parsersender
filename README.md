# Инструмент парсинга логов.

Здесь предлагается подход к парсингу логов

В качестве примера прилагается proof-of-concept парсер/загрузчик логов nginx с фронтендов.


## Что это такое?

Есть 2 части этого инструмента:

### 1. Parser

Он занимается чтением stdin, парсингом и складыванием в csv файлы лога в указанную папку. csv затем быстро можно грузить в кликхаус. Дальше их можно агрегировать, анализировать, отображать в графане и т д. Старые можно дропать.

Парсер написан на awk.

Почему выбор пал именно на awk? По нескольким причинам

1. Он очень простой, любой разработчик или админ скорее всего разберется что там написано
2. Он довольно производительный
3. Парсеры на нём писать приятно
4. Он есть везде, на любой машине с POSIX-совместимой системой, даже в докере

## 2. Sender

Это также очень простая штука: она смотрит в указанную папку и раз в 5 секунд грузить в кликхаус все файлы кроме последнего(в них гарантировано никто не пишет), загруженные файлы удаляет.

Также добавлен пример конфига supervisord который следит за состоянием процессов и перезапускает в случае падения.

---

## Производительность

Лучше всего показал результаты mawk(имплементация awk, дефолтная на Ubuntu/Debian);
mawk прекомпилируется перед запуском

gawk(дефолтная на centos) похуже, но тоже вполне прилично:
gawk интерпретируется как обычный скриптовый язык

Фронтенд-сервера при этом практически никакого влияния не оказывается, ни по нагрузке на CPU, ни на сеть.

## Недостатки подхода

1. awk не умеет в многопоточность
2. потенциально производительности awk может не хватить, но оптимальный
парсер надо уже тогда писать не на скриптовом языке

## Можно улучшать

Можно не использовать ssh для забора логов, а, например, rsync.

Можно использовать вместо supervisor что-нибудь поинтереснее.

Можно оптимизировать и стандартизировать логи, чтобы парсить было ещё проще и быстрее.
